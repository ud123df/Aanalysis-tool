{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1bd7161-6351-4333-a693-30088dd2aa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\tkinter\\__init__.py\", line 2074, in __call__\n",
      "    return self.func(*args)\n",
      "           ~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\UDIT\\AppData\\Local\\Temp\\ipykernel_8960\\3024515263.py\", line 249, in apply_iqr\n",
      "    q1 = self.filtered_df[col].quantile(0.25)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\series.py\", line 2898, in quantile\n",
      "    result = df.quantile(q=q, interpolation=interpolation, numeric_only=False)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\frame.py\", line 12153, in quantile\n",
      "    res_df = self.quantile(\n",
      "        [q],  # type: ignore[list-item]\n",
      "    ...<3 lines>...\n",
      "        method=method,\n",
      "    )\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\frame.py\", line 12198, in quantile\n",
      "    res = data._mgr.quantile(qs=q, interpolation=interpolation)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 1549, in quantile\n",
      "    blk.quantile(qs=qs, interpolation=interpolation) for blk in self.blocks\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 1957, in quantile\n",
      "    result = quantile_compat(self.values, np.asarray(qs._values), interpolation)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\array_algos\\quantile.py\", line 39, in quantile_compat\n",
      "    return quantile_with_mask(values, mask, fill_value, qs, interpolation)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\array_algos\\quantile.py\", line 97, in quantile_with_mask\n",
      "    result = _nanpercentile(\n",
      "        values,\n",
      "    ...<3 lines>...\n",
      "        interpolation=interpolation,\n",
      "    )\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\array_algos\\quantile.py\", line 218, in _nanpercentile\n",
      "    return np.percentile(\n",
      "           ~~~~~~~~~~~~~^\n",
      "        values,\n",
      "        ^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        method=interpolation,  # type: ignore[call-overload]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py\", line 4273, in percentile\n",
      "    return _quantile_unchecked(\n",
      "        a, q, axis, out, overwrite_input, method, keepdims, weights)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py\", line 4550, in _quantile_unchecked\n",
      "    return _ureduce(a,\n",
      "                    func=_quantile_ureduce_func,\n",
      "    ...<5 lines>...\n",
      "                    overwrite_input=overwrite_input,\n",
      "                    method=method)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py\", line 3894, in _ureduce\n",
      "    r = func(a, **kwargs)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py\", line 4727, in _quantile_ureduce_func\n",
      "    result = _quantile(arr,\n",
      "                       quantiles=q,\n",
      "    ...<2 lines>...\n",
      "                       out=out,\n",
      "                       weights=wgt)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py\", line 4859, in _quantile\n",
      "    result = _lerp(previous,\n",
      "                next,\n",
      "                gamma,\n",
      "                out=out)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py\", line 4653, in _lerp\n",
      "    diff_b_a = subtract(b, a)\n",
      "TypeError: unsupported operand type(s) for -: 'str' and 'str'\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\tkinter\\__init__.py\", line 2074, in __call__\n",
      "    return self.func(*args)\n",
      "           ~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\UDIT\\AppData\\Local\\Temp\\ipykernel_8960\\3024515263.py\", line 249, in apply_iqr\n",
      "    q1 = self.filtered_df[col].quantile(0.25)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\series.py\", line 2898, in quantile\n",
      "    result = df.quantile(q=q, interpolation=interpolation, numeric_only=False)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\frame.py\", line 12153, in quantile\n",
      "    res_df = self.quantile(\n",
      "        [q],  # type: ignore[list-item]\n",
      "    ...<3 lines>...\n",
      "        method=method,\n",
      "    )\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\frame.py\", line 12198, in quantile\n",
      "    res = data._mgr.quantile(qs=q, interpolation=interpolation)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 1549, in quantile\n",
      "    blk.quantile(qs=qs, interpolation=interpolation) for blk in self.blocks\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 1957, in quantile\n",
      "    result = quantile_compat(self.values, np.asarray(qs._values), interpolation)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\array_algos\\quantile.py\", line 39, in quantile_compat\n",
      "    return quantile_with_mask(values, mask, fill_value, qs, interpolation)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\array_algos\\quantile.py\", line 97, in quantile_with_mask\n",
      "    result = _nanpercentile(\n",
      "        values,\n",
      "    ...<3 lines>...\n",
      "        interpolation=interpolation,\n",
      "    )\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pandas\\core\\array_algos\\quantile.py\", line 218, in _nanpercentile\n",
      "    return np.percentile(\n",
      "           ~~~~~~~~~~~~~^\n",
      "        values,\n",
      "        ^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        method=interpolation,  # type: ignore[call-overload]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py\", line 4273, in percentile\n",
      "    return _quantile_unchecked(\n",
      "        a, q, axis, out, overwrite_input, method, keepdims, weights)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py\", line 4550, in _quantile_unchecked\n",
      "    return _ureduce(a,\n",
      "                    func=_quantile_ureduce_func,\n",
      "    ...<5 lines>...\n",
      "                    overwrite_input=overwrite_input,\n",
      "                    method=method)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py\", line 3894, in _ureduce\n",
      "    r = func(a, **kwargs)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py\", line 4727, in _quantile_ureduce_func\n",
      "    result = _quantile(arr,\n",
      "                       quantiles=q,\n",
      "    ...<2 lines>...\n",
      "                       out=out,\n",
      "                       weights=wgt)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py\", line 4859, in _quantile\n",
      "    result = _lerp(previous,\n",
      "                next,\n",
      "                gamma,\n",
      "                out=out)\n",
      "  File \"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py\", line 4653, in _lerp\n",
      "    diff_b_a = subtract(b, a)\n",
      "TypeError: unsupported operand type(s) for -: 'str' and 'str'\n",
      "C:\\Users\\UDIT\\AppData\\Local\\Temp\\ipykernel_8960\\3024515263.py:365: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  self.ax.set_xticklabels(group.index.astype(str), rotation=45, ha=\"right\")\n",
      "C:\\Users\\UDIT\\AppData\\Local\\Temp\\ipykernel_8960\\3024515263.py:365: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  self.ax.set_xticklabels(group.index.astype(str), rotation=45, ha=\"right\")\n",
      "C:\\Users\\UDIT\\AppData\\Local\\Temp\\ipykernel_8960\\3024515263.py:365: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  self.ax.set_xticklabels(group.index.astype(str), rotation=45, ha=\"right\")\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, filedialog, messagebox\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2Tk\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "# Optional AutoML\n",
    "AUTO_ML_AVAILABLE = True\n",
    "TPOT_AVAILABLE = True\n",
    "try:\n",
    "    from autosklearn.classification import AutoSklearnClassifier\n",
    "    from autosklearn.regression import AutoSklearnRegressor\n",
    "except Exception:\n",
    "    AUTO_ML_AVAILABLE = False\n",
    "try:\n",
    "    from tpot import TPOTClassifier, TPOTRegressor\n",
    "except Exception:\n",
    "    TPOT_AVAILABLE = False\n",
    "\n",
    "class BusinessAnalystApp(tk.Tk):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title(\"Business Analyst GUI\")\n",
    "        self.geometry(\"1100x700\")\n",
    "        self.df = None\n",
    "        self.filtered_df = None\n",
    "        self.target_col = None\n",
    "\n",
    "        # Notebook\n",
    "        self.nb = ttk.Notebook(self)\n",
    "        self.nb.pack(fill=\"both\", expand=True)\n",
    "\n",
    "        # Tabs\n",
    "        self.tab_data = ttk.Frame(self.nb)\n",
    "        self.tab_clean = ttk.Frame(self.nb)\n",
    "        self.tab_viz = ttk.Frame(self.nb)\n",
    "        self.tab_model = ttk.Frame(self.nb)\n",
    "\n",
    "        self.nb.add(self.tab_data, text=\"Data\")\n",
    "        self.nb.add(self.tab_clean, text=\"Clean/Transform\")\n",
    "        self.nb.add(self.tab_viz, text=\"Visualize\")\n",
    "        self.nb.add(self.tab_model, text=\"Model\")\n",
    "\n",
    "        self.build_data_tab()\n",
    "        self.build_clean_tab()\n",
    "        self.build_viz_tab()\n",
    "        self.build_model_tab()\n",
    "\n",
    "    # === Data Tab ===\n",
    "    def build_data_tab(self):\n",
    "        top = ttk.Frame(self.tab_data)\n",
    "        top.pack(fill=\"x\", pady=5, padx=5)\n",
    "\n",
    "        ttk.Button(top, text=\"Open CSV/Excel\", command=self.open_file).pack(side=\"left\")\n",
    "        ttk.Button(top, text=\"Save Current View\", command=self.save_current).pack(side=\"left\", padx=8)\n",
    "        self.info_lbl = ttk.Label(top, text=\"No file loaded\")\n",
    "        self.info_lbl.pack(side=\"left\", padx=12)\n",
    "\n",
    "        # Treeview for preview\n",
    "        frame = ttk.Frame(self.tab_data)\n",
    "        frame.pack(fill=\"both\", expand=True, padx=5, pady=5)\n",
    "        self.tree = ttk.Treeview(frame, show=\"headings\")\n",
    "        self.tree.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "        vsb = ttk.Scrollbar(frame, orient=\"vertical\", command=self.tree.yview)\n",
    "        hsb = ttk.Scrollbar(frame, orient=\"horizontal\", command=self.tree.xview)\n",
    "        self.tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)\n",
    "        vsb.pack(side=\"right\", fill=\"y\")\n",
    "        hsb.pack(side=\"bottom\", fill=\"x\")\n",
    "\n",
    "    def open_file(self):\n",
    "        path = filedialog.askopenfilename(title=\"Select data file\",\n",
    "                                          filetypes=[(\"CSV\", \"*.csv\"), (\"Excel\", \"*.xlsx *.xls\"), (\"All files\",\"*.*\")])\n",
    "        if not path:\n",
    "            return\n",
    "        try:\n",
    "            if path.lower().endswith(\".csv\"):\n",
    "                self.df = pd.read_csv(path)\n",
    "            else:\n",
    "                self.df = pd.read_excel(path)\n",
    "            self.filtered_df = self.df.copy()\n",
    "            self.info_lbl.config(text=f\"Loaded: {os.path.basename(path)} | rows={len(self.df)} cols={self.df.shape[1]}\")\n",
    "            self.refresh_table(self.filtered_df.head(500))\n",
    "            self.refresh_column_controls()\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Load error\", str(e))\n",
    "\n",
    "    def save_current(self):\n",
    "        if self.filtered_df is None:\n",
    "            messagebox.showwarning(\"No data\", \"Load data first\")\n",
    "            return\n",
    "        path = filedialog.asksaveasfilename(defaultextension=\".csv\",\n",
    "                                            filetypes=[(\"CSV\", \"*.csv\")])\n",
    "        if not path:\n",
    "            return\n",
    "        try:\n",
    "            self.filtered_df.to_csv(path, index=False)\n",
    "            messagebox.showinfo(\"Saved\", f\"Saved to {path}\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Save error\", str(e))\n",
    "\n",
    "    def refresh_table(self, df):\n",
    "        # clear\n",
    "        self.tree.delete(*self.tree.get_children())\n",
    "        self.tree[\"columns\"] = list(df.columns)\n",
    "        for c in df.columns:\n",
    "            self.tree.heading(c, text=c)\n",
    "            self.tree.column(c, width=150, anchor=\"w\")\n",
    "        for _, row in df.iterrows():\n",
    "            self.tree.insert(\"\", \"end\", values=[row[c] for c in df.columns])\n",
    "\n",
    "    # === Clean/Transform Tab ===\n",
    "    def build_clean_tab(self):\n",
    "        pane = ttk.Panedwindow(self.tab_clean, orient=\"horizontal\")\n",
    "        pane.pack(fill=\"both\", expand=True, padx=5, pady=5)\n",
    "\n",
    "        left = ttk.Frame(pane, width=360)\n",
    "        right = ttk.Frame(pane)\n",
    "        pane.add(left, weight=1)\n",
    "        pane.add(right, weight=3)\n",
    "\n",
    "        # Cleaning controls\n",
    "        ttk.Label(left, text=\"Cleaning\").pack(anchor=\"w\", pady=(0,6))\n",
    "        self.dropna_btn = ttk.Button(left, text=\"Drop rows with nulls\", command=self.dropna_rows)\n",
    "        self.dropna_btn.pack(fill=\"x\", pady=2)\n",
    "\n",
    "        frm_fill = ttk.Frame(left)\n",
    "        frm_fill.pack(fill=\"x\", pady=2)\n",
    "        ttk.Label(frm_fill, text=\"Fill NA by:\").pack(side=\"left\")\n",
    "        self.fill_method = tk.StringVar(value=\"mean\")\n",
    "        ttk.Combobox(frm_fill, textvariable=self.fill_method, values=[\"mean\",\"median\",\"mode\",\"constant\"]).pack(side=\"left\", padx=6)\n",
    "        self.fill_constant = tk.StringVar()\n",
    "        ttk.Entry(frm_fill, textvariable=self.fill_constant, width=8).pack(side=\"left\")\n",
    "        ttk.Button(left, text=\"Apply Fill\", command=self.fill_na).pack(fill=\"x\", pady=2)\n",
    "\n",
    "        ttk.Label(left, text=\"Drop Column\").pack(anchor=\"w\", pady=(8,2))\n",
    "        self.drop_col = tk.StringVar()\n",
    "        self.drop_col_cb = ttk.Combobox(left, textvariable=self.drop_col)\n",
    "        self.drop_col_cb.pack(fill=\"x\")\n",
    "        ttk.Button(left, text=\"Drop\", command=self.drop_column).pack(fill=\"x\", pady=2)\n",
    "\n",
    "        ttk.Label(left, text=\"Remove Outliers (IQR) on column\").pack(anchor=\"w\", pady=(8,2))\n",
    "        self.iqr_col = tk.StringVar()\n",
    "        self.iqr_col_cb = ttk.Combobox(left, textvariable=self.iqr_col)\n",
    "        self.iqr_col_cb.pack(fill=\"x\")\n",
    "        ttk.Button(left, text=\"Apply IQR Filter\", command=self.apply_iqr).pack(fill=\"x\", pady=2)\n",
    "\n",
    "        ttk.Separator(left).pack(fill=\"x\", pady=8)\n",
    "\n",
    "        ttk.Label(left, text=\"Transform\").pack(anchor=\"w\", pady=(0,6))\n",
    "        self.filter_expr = tk.StringVar()\n",
    "        ttk.Entry(left, textvariable=self.filter_expr).pack(fill=\"x\")\n",
    "        ttk.Label(left, text=\"Example: Sales > 1000 and Region == 'East'\").pack(anchor=\"w\")\n",
    "        ttk.Button(left, text=\"Apply Row Filter\", command=self.apply_filter).pack(fill=\"x\", pady=2)\n",
    "\n",
    "        self.newcol_name = tk.StringVar()\n",
    "        self.newcol_expr = tk.StringVar()\n",
    "        ttk.Entry(left, textvariable=self.newcol_name).pack(fill=\"x\")\n",
    "        self.newcol_name.set(\"NewColumnName\")\n",
    "        ttk.Entry(left, textvariable=self.newcol_expr).pack(fill=\"x\")\n",
    "        self.newcol_expr.set(\"Quantity * Price\")\n",
    "        ttk.Button(left, text=\"Create Computed Column\", command=self.create_column).pack(fill=\"x\", pady=2)\n",
    "\n",
    "        ttk.Label(left, text=\"Groupby Agg\").pack(anchor=\"w\", pady=(8,2))\n",
    "        self.groupby_cols = tk.StringVar()\n",
    "        self.groupby_aggs = tk.StringVar()\n",
    "        ttk.Entry(left, textvariable=self.groupby_cols).pack(fill=\"x\")\n",
    "        self.groupby_cols.set(\"Region, Category\")\n",
    "        ttk.Entry(left, textvariable=self.groupby_aggs).pack(fill=\"x\")\n",
    "        self.groupby_aggs.set(\"Sales:sum, Quantity:mean\")\n",
    "        ttk.Button(left, text=\"Run Groupby\", command=self.run_groupby).pack(fill=\"x\", pady=2)\n",
    "\n",
    "        # Right preview table\n",
    "        self.clean_tree = ttk.Treeview(right, show=\"headings\")\n",
    "        self.clean_tree.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "        vsb = ttk.Scrollbar(right, orient=\"vertical\", command=self.clean_tree.yview)\n",
    "        hsb = ttk.Scrollbar(right, orient=\"horizontal\", command=self.clean_tree.xview)\n",
    "        self.clean_tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)\n",
    "        vsb.pack(side=\"right\", fill=\"y\")\n",
    "        hsb.pack(side=\"bottom\", fill=\"x\")\n",
    "\n",
    "    def refresh_clean_preview(self):\n",
    "        if self.filtered_df is not None:\n",
    "            self._refresh_tree(self.clean_tree, self.filtered_df.head(500))\n",
    "\n",
    "    def _refresh_tree(self, tree, df):\n",
    "        tree.delete(*tree.get_children())\n",
    "        tree[\"columns\"] = list(df.columns)\n",
    "        for c in df.columns:\n",
    "            tree.heading(c, text=c)\n",
    "            tree.column(c, width=150, anchor=\"w\")\n",
    "        for _, row in df.iterrows():\n",
    "            tree.insert(\"\", \"end\", values=[row[c] for c in df.columns])\n",
    "\n",
    "    def refresh_column_controls(self):\n",
    "        if self.filtered_df is None:\n",
    "            return\n",
    "        cols = list(self.filtered_df.columns)\n",
    "        self.drop_col_cb[\"values\"] = cols\n",
    "        self.iqr_col_cb[\"values\"] = cols\n",
    "        self.viz_x_cb[\"values\"] = cols\n",
    "        self.viz_y_cb[\"values\"] = cols\n",
    "        self.model_target_cb[\"values\"] = cols\n",
    "\n",
    "    def dropna_rows(self):\n",
    "        if self.filtered_df is None: return\n",
    "        self.filtered_df = self.filtered_df.dropna()\n",
    "        self.refresh_clean_preview()\n",
    "\n",
    "    def fill_na(self):\n",
    "        if self.filtered_df is None: return\n",
    "        method = self.fill_method.get()\n",
    "        if method in (\"mean\",\"median\"):\n",
    "            nums = self.filtered_df.select_dtypes(include=np.number).columns\n",
    "            if method == \"mean\":\n",
    "                self.filtered_df[nums] = self.filtered_df[nums].fillna(self.filtered_df[nums].mean())\n",
    "            else:\n",
    "                self.filtered_df[nums] = self.filtered_df[nums].fillna(self.filtered_df[nums].median())\n",
    "        elif method == \"mode\":\n",
    "            self.filtered_df = self.filtered_df.fillna(self.filtered_df.mode().iloc[0])\n",
    "        else:\n",
    "            val = self.fill_constant.get()\n",
    "            self.filtered_df = self.filtered_df.fillna(val)\n",
    "        self.refresh_clean_preview()\n",
    "\n",
    "    def drop_column(self):\n",
    "        if self.filtered_df is None: return\n",
    "        col = self.drop_col.get()\n",
    "        if col and col in self.filtered_df.columns:\n",
    "            self.filtered_df = self.filtered_df.drop(columns=[col])\n",
    "            self.refresh_column_controls()\n",
    "            self.refresh_clean_preview()\n",
    "\n",
    "    def apply_iqr(self):\n",
    "        if self.filtered_df is None: return\n",
    "        col = self.iqr_col.get()\n",
    "        if col not in self.filtered_df.columns:\n",
    "            return\n",
    "        q1 = self.filtered_df[col].quantile(0.25)\n",
    "        q3 = self.filtered_df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - 1.5*iqr\n",
    "        upper = q3 + 1.5*iqr\n",
    "        self.filtered_df = self.filtered_df[(self.filtered_df[col] >= lower) & (self.filtered_df[col] <= upper)]\n",
    "        self.refresh_clean_preview()\n",
    "\n",
    "    def apply_filter(self):\n",
    "        if self.filtered_df is None: return\n",
    "        expr = self.filter_expr.get().strip()\n",
    "        if not expr:\n",
    "            return\n",
    "        try:\n",
    "            self.filtered_df = self.filtered_df.query(expr)\n",
    "            self.refresh_clean_preview()\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Filter error\", str(e))\n",
    "\n",
    "    def create_column(self):\n",
    "        if self.filtered_df is None: return\n",
    "        name = self.newcol_name.get().strip()\n",
    "        expr = self.newcol_expr.get().strip()\n",
    "        if not name or not expr:\n",
    "            return\n",
    "        try:\n",
    "            self.filtered_df[name] = self.filtered_df.eval(expr)\n",
    "            self.refresh_column_controls()\n",
    "            self.refresh_clean_preview()\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Expression error\", str(e))\n",
    "\n",
    "    def run_groupby(self):\n",
    "        if self.filtered_df is None: return\n",
    "        try:\n",
    "            cols = [c.strip() for c in self.groupby_cols.get().split(\",\") if c.strip()]\n",
    "            agg_pairs = [p.strip() for p in self.groupby_aggs.get().split(\",\") if p.strip()]\n",
    "            agg_map = {}\n",
    "            for p in agg_pairs:\n",
    "                col, fn = p.split(\":\")\n",
    "                agg_map[col.strip()] = fn.strip()\n",
    "            g = self.filtered_df.groupby(cols).agg(agg_map).reset_index()\n",
    "            self.filtered_df = g\n",
    "            self.refresh_column_controls()\n",
    "            self.refresh_clean_preview()\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Groupby error\", str(e))\n",
    "\n",
    "    # === Visualization Tab ===\n",
    "    def build_viz_tab(self):\n",
    "        left = ttk.Frame(self.tab_viz, width=260)\n",
    "        left.pack(side=\"left\", fill=\"y\", padx=5, pady=5)\n",
    "        right = ttk.Frame(self.tab_viz)\n",
    "        right.pack(side=\"left\", fill=\"both\", expand=True, padx=5, pady=5)\n",
    "\n",
    "        ttk.Label(left, text=\"Chart Type\").pack(anchor=\"w\")\n",
    "        self.viz_type = tk.StringVar(value=\"bar\")\n",
    "        ttk.Combobox(left, textvariable=self.viz_type, values=[\"bar\",\"line\",\"hist\",\"scatter\"]).pack(fill=\"x\", pady=2)\n",
    "\n",
    "        ttk.Label(left, text=\"X Column\").pack(anchor=\"w\", pady=(6,0))\n",
    "        self.viz_x = tk.StringVar()\n",
    "        self.viz_x_cb = ttk.Combobox(left, textvariable=self.viz_x)\n",
    "        self.viz_x_cb.pack(fill=\"x\")\n",
    "\n",
    "        ttk.Label(left, text=\"Y Column\").pack(anchor=\"w\", pady=(6,0))\n",
    "        self.viz_y = tk.StringVar()\n",
    "        self.viz_y_cb = ttk.Combobox(left, textvariable=self.viz_y)\n",
    "        self.viz_y_cb.pack(fill=\"x\")\n",
    "\n",
    "        ttk.Label(left, text=\"Bins (hist)\").pack(anchor=\"w\", pady=(6,0))\n",
    "        self.viz_bins = tk.IntVar(value=20)\n",
    "        ttk.Entry(left, textvariable=self.viz_bins).pack(fill=\"x\")\n",
    "\n",
    "        ttk.Button(left, text=\"Plot\", command=self.plot_chart).pack(fill=\"x\", pady=8)\n",
    "\n",
    "        # Matplotlib figure embedded\n",
    "        self.fig = Figure(figsize=(7,5), dpi=100)\n",
    "        self.ax = self.fig.add_subplot(111)\n",
    "        self.canvas = FigureCanvasTkAgg(self.fig, master=right)\n",
    "        self.canvas.draw()\n",
    "        self.canvas.get_tk_widget().pack(fill=\"both\", expand=True)\n",
    "        self.toolbar = NavigationToolbar2Tk(self.canvas, right)\n",
    "        self.toolbar.update()\n",
    "\n",
    "    def plot_chart(self):\n",
    "        if self.filtered_df is None:\n",
    "            messagebox.showwarning(\"No data\", \"Load data first\")\n",
    "            return\n",
    "        self.ax.clear()\n",
    "        chart = self.viz_type.get()\n",
    "        x = self.viz_x.get()\n",
    "        y = self.viz_y.get()\n",
    "\n",
    "        try:\n",
    "            if chart == \"hist\":\n",
    "                if y and y in self.filtered_df.columns:\n",
    "                    self.ax.hist(self.filtered_df[y].dropna(), bins=self.viz_bins.get(), color=\"#4e79a7\")\n",
    "                else:\n",
    "                    # try all numeric\n",
    "                    nums = self.filtered_df.select_dtypes(include=np.number)\n",
    "                    if nums.shape[1] == 0:\n",
    "                        messagebox.showwarning(\"No numeric\", \"No numeric columns for histogram\")\n",
    "                        return\n",
    "                    self.ax.hist(nums.iloc[:,0].dropna(), bins=self.viz_bins.get(), color=\"#4e79a7\")\n",
    "                self.ax.set_title(\"Histogram\")\n",
    "            elif chart == \"scatter\":\n",
    "                if x in self.filtered_df.columns and y in self.filtered_df.columns:\n",
    "                    self.ax.scatter(self.filtered_df[x], self.filtered_df[y], alpha=0.7, color=\"#59a14f\")\n",
    "                    self.ax.set_xlabel(x); self.ax.set_ylabel(y)\n",
    "                    self.ax.set_title(f\"Scatter: {x} vs {y}\")\n",
    "                else:\n",
    "                    messagebox.showwarning(\"Columns\", \"Select valid X and Y columns\")\n",
    "            elif chart == \"bar\":\n",
    "                if x and y and x in self.filtered_df.columns and y in self.filtered_df.columns:\n",
    "                    group = self.filtered_df.groupby(x)[y].sum().sort_values(ascending=False).head(20)\n",
    "                    self.ax.bar(group.index.astype(str), group.values, color=\"#e15759\")\n",
    "                    self.ax.set_xticklabels(group.index.astype(str), rotation=45, ha=\"right\")\n",
    "                    self.ax.set_title(f\"Bar: {y} by {x}\")\n",
    "                else:\n",
    "                    messagebox.showwarning(\"Columns\", \"Select valid X and Y columns\")\n",
    "            else:  # line\n",
    "                if x and y and x in self.filtered_df.columns and y in self.filtered_df.columns:\n",
    "                    self.ax.plot(self.filtered_df[x], self.filtered_df[y], color=\"#f28e2b\")\n",
    "                    self.ax.set_xlabel(x); self.ax.set_ylabel(y)\n",
    "                    self.ax.set_title(f\"Line: {y} over {x}\")\n",
    "                else:\n",
    "                    messagebox.showwarning(\"Columns\", \"Select valid X and Y columns\")\n",
    "\n",
    "            self.fig.tight_layout()\n",
    "            self.canvas.draw()\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Plot error\", str(e))\n",
    "\n",
    "    # === Modeling Tab ===\n",
    "    def build_model_tab(self):\n",
    "        left = ttk.Frame(self.tab_model, width=260)\n",
    "        left.pack(side=\"left\", fill=\"y\", padx=5, pady=5)\n",
    "        right = ttk.Frame(self.tab_model)\n",
    "        right.pack(side=\"left\", fill=\"both\", expand=True, padx=5, pady=5)\n",
    "\n",
    "        ttk.Label(left, text=\"Target Column\").pack(anchor=\"w\")\n",
    "        self.model_target = tk.StringVar()\n",
    "        self.model_target_cb = ttk.Combobox(left, textvariable=self.model_target)\n",
    "        self.model_target_cb.pack(fill=\"x\", pady=2)\n",
    "\n",
    "        ttk.Label(left, text=\"Task\").pack(anchor=\"w\", pady=(6,0))\n",
    "        self.task_type = tk.StringVar(value=\"auto\")\n",
    "        ttk.Combobox(left, textvariable=self.task_type, values=[\"auto\",\"classification\",\"regression\"]).pack(fill=\"x\")\n",
    "\n",
    "        ttk.Button(left, text=\"Train Baseline Model\", command=self.train_model).pack(fill=\"x\", pady=8)\n",
    "        ttk.Button(left, text=\"Run AutoML (if available)\", command=self.train_automl).pack(fill=\"x\", pady=4)\n",
    "\n",
    "        self.model_status = tk.Text(right, height=20)\n",
    "        self.model_status.pack(fill=\"both\", expand=True)\n",
    "\n",
    "    def log(self, msg):\n",
    "        self.model_status.insert(\"end\", msg + \"\\n\")\n",
    "        self.model_status.see(\"end\")\n",
    "        self.update_idletasks()\n",
    "\n",
    "    def infer_task(self, y):\n",
    "        # heuristic: numeric -> regression, else classification\n",
    "        return \"regression\" if pd.api.types.is_numeric_dtype(y) else \"classification\"\n",
    "\n",
    "    def train_model(self):\n",
    "        if self.filtered_df is None:\n",
    "            messagebox.showwarning(\"No data\", \"Load and prepare data\")\n",
    "            return\n",
    "        target = self.model_target.get()\n",
    "        if target not in self.filtered_df.columns:\n",
    "            messagebox.showwarning(\"Target\", \"Select a valid target\")\n",
    "            return\n",
    "        df = self.filtered_df.dropna(subset=[target]).copy()\n",
    "        y = df[target]\n",
    "        X = df.drop(columns=[target])\n",
    "\n",
    "        task = self.task_type.get()\n",
    "        if task == \"auto\":\n",
    "            task = self.infer_task(y)\n",
    "\n",
    "        # Preprocess: numeric impute + categorical one-hot\n",
    "        num_cols = X.select_dtypes(include=np.number).columns\n",
    "        cat_cols = X.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "        num_pipe = Pipeline(steps=[(\"impute\", SimpleImputer(strategy=\"median\"))])\n",
    "        cat_pipe = Pipeline(steps=[(\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                                   (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "        pre = ColumnTransformer([\n",
    "            (\"num\", num_pipe, num_cols),\n",
    "            (\"cat\", cat_pipe, cat_cols)\n",
    "        ])\n",
    "\n",
    "        if task == \"classification\":\n",
    "            model = LogisticRegression(max_iter=1000)\n",
    "            metric_fn = accuracy_score\n",
    "        else:\n",
    "            model = LinearRegression()\n",
    "            metric_fn = r2_score\n",
    "\n",
    "        pipe = Pipeline([(\"pre\", pre), (\"model\", model)])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.log(\"Training baseline model...\")\n",
    "        pipe.fit(X_train, y_train)\n",
    "        preds = pipe.predict(X_test)\n",
    "        score = metric_fn(y_test, preds if task == \"regression\" else (preds > 0.5 if preds.ndim == 1 else np.argmax(preds, axis=1)))\n",
    "        self.log(f\"Task: {task} | Score: {score:.4f}\")\n",
    "        self.log(\"Done.\\n\")\n",
    "\n",
    "    def train_automl(self):\n",
    "        if self.filtered_df is None:\n",
    "            messagebox.showwarning(\"No data\", \"Load and prepare data\")\n",
    "            return\n",
    "        target = self.model_target.get()\n",
    "        if target not in self.filtered_df.columns:\n",
    "            messagebox.showwarning(\"Target\", \"Select a valid target\")\n",
    "            return\n",
    "\n",
    "        df = self.filtered_df.dropna(subset=[target]).copy()\n",
    "        y = df[target]\n",
    "        X = df.drop(columns=[target])\n",
    "\n",
    "        task = self.task_type.get()\n",
    "        if task == \"auto\":\n",
    "            task = self.infer_task(y)\n",
    "\n",
    "        # Basic preprocessing similar to baseline\n",
    "        num_cols = X.select_dtypes(include=np.number).columns\n",
    "        cat_cols = X.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "        pre = ColumnTransformer([\n",
    "            (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
    "            (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                              (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols)\n",
    "        ])\n",
    "\n",
    "        X_proc = pre.fit_transform(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_proc, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        if AUTO_ML_AVAILABLE:\n",
    "            self.log(\"AutoML: auto-sklearn running...\")\n",
    "            if task == \"classification\":\n",
    "                automl = AutoSklearnClassifier(time_left_for_this_task=120, per_run_time_limit=30)\n",
    "            else:\n",
    "                automl = AutoSklearnRegressor(time_left_for_this_task=120, per_run_time_limit=30)\n",
    "            automl.fit(X_train, y_train)\n",
    "            preds = automl.predict(X_test)\n",
    "            score = (accuracy_score(y_test, preds) if task == \"classification\" else r2_score(y_test, preds))\n",
    "            self.log(f\"AutoML score: {score:.4f}\")\n",
    "        elif TPOT_AVAILABLE:\n",
    "            self.log(\"AutoML: TPOT running...\")\n",
    "            if task == \"classification\":\n",
    "                automl = TPOTClassifier(generations=5, population_size=20, verbosity=2, max_time_mins=2)\n",
    "            else:\n",
    "                automl = TPOTRegressor(generations=5, population_size=20, verbosity=2, max_time_mins=2)\n",
    "            automl.fit(X_train, y_train)\n",
    "            preds = automl.predict(X_test)\n",
    "            score = (accuracy_score(y_test, preds) if task == \"classification\" else r2_score(y_test, preds))\n",
    "            self.log(f\"TPOT score: {score:.4f}\")\n",
    "            try:\n",
    "                automl.export(\"automl_pipeline.py\")\n",
    "                self.log(\"Exported pipeline to automl_pipeline.py\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        else:\n",
    "            self.log(\"AutoML not available. Install auto-sklearn or tpot.\")\n",
    "        self.log(\"Done.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = BusinessAnalystApp()\n",
    "    app.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd3783fc-520e-4a2d-a800-e886a2dee571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting auto-sklearn\n",
      "  Using cached auto-sklearn-0.15.0.tar.gz (6.5 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [23 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \u001b[35m\"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "      \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "      json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "                               \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\UDIT\\anaconda3\\envs\\sklearn\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "      return hook(config_settings)\n",
      "    File \u001b[35m\"C:\\Users\\UDIT\\AppData\\Local\\Temp\\pip-build-env-y92mzgqq\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "      return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "             \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\UDIT\\AppData\\Local\\Temp\\pip-build-env-y92mzgqq\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "      \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\UDIT\\AppData\\Local\\Temp\\pip-build-env-y92mzgqq\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "      \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\UDIT\\AppData\\Local\\Temp\\pip-build-env-y92mzgqq\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "      \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "      \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m10\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[1;35mValueError\u001b[0m: \u001b[35mDetected unsupported operating system: win32. Please check the compability information of auto-sklearn: https://automl.github.io/auto-sklearn/master/installation.html#windows-osx-compatibility\u001b[0m\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "# pip install auto-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48818763-3473-4a16-968c-c5fb337680f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903c542-e54c-48f5-900f-8161a02f1417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a45c3b-fbe3-4877-9430-029ad9e8a658",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ydata_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# EDA / Profiling\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mydata_profiling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProfileReport  \u001b[38;5;66;03m# pip install ydata-profiling\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Visualizations\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ydata_profiling'"
     ]
    }
   ],
   "source": [
    "# # business_analyst_agent.py\n",
    "# import os, time, json, re, warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "\n",
    "# # EDA / Profiling\n",
    "# from ydata_profiling import ProfileReport  # pip install ydata-profiling\n",
    "\n",
    "# # Visualizations\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # ML / AutoML\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, r2_score\n",
    "# import joblib\n",
    "\n",
    "# # Optional AutoML (classification/regression)\n",
    "# try:\n",
    "#     import autosklearn.classification as ask_cls\n",
    "#     import autosklearn.regression as ask_reg\n",
    "#     AUTOSKLEARN_AVAILABLE = True\n",
    "# except Exception:\n",
    "#     AUTOSKLEARN_AVAILABLE = False\n",
    "\n",
    "# # Web mining (ethical scraping)\n",
    "# import requests\n",
    "# from urllib.parse import urljoin, urlparse\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # ------------------ Utility ------------------\n",
    "\n",
    "# def ensure_dir(path: str):\n",
    "#     Path(path).mkdir(parents=True, exist_ok=True)\n",
    "#     return path\n",
    "\n",
    "# def safe_filename(name: str):\n",
    "#     return re.sub(r\"[^a-zA-Z0-9_.-]\", \"_\", name)\n",
    "\n",
    "# # ------------------ Data Ingestion ------------------\n",
    "\n",
    "# def load_data(input_path_or_url: str) -> pd.DataFrame:\n",
    "#     if re.match(r\"^https?://\", input_path_or_url, flags=re.I):\n",
    "#         # Try CSV first\n",
    "#         try:\n",
    "#             return pd.read_csv(input_path_or_url)\n",
    "#         except Exception:\n",
    "#             pass\n",
    "#         # Try HTML tables\n",
    "#         try:\n",
    "#             tables = pd.read_html(input_path_or_url)\n",
    "#             if tables:\n",
    "#                 return tables[0]\n",
    "#         except Exception:\n",
    "#             pass\n",
    "#         raise ValueError(\"Unsupported URL or no table/CSV found at URL.\")\n",
    "#     else:\n",
    "#         ext = Path(input_path_or_url).suffix.lower()\n",
    "#         if ext in [\".csv\"]:\n",
    "#             return pd.read_csv(input_path_or_url)\n",
    "#         if ext in [\".xlsx\", \".xls\"]:\n",
    "#             return pd.read_excel(input_path_or_url)\n",
    "#         raise ValueError(\"Unsupported file format. Use CSV/XLSX or a URL to a CSV/table.\")\n",
    "\n",
    "# # ------------------ Profiling & Cleaning ------------------\n",
    "\n",
    "# def profile_and_clean(df: pd.DataFrame, out_dir=\"outputs/eda\", title=\"EDA Report\") -> pd.DataFrame:\n",
    "#     ensure_dir(out_dir)\n",
    "#     # Profiling report\n",
    "#     profile = ProfileReport(df, title=title, explorative=True)\n",
    "#     profile_path = os.path.join(out_dir, safe_filename(title) + \".html\")\n",
    "#     profile.to_file(profile_path)\n",
    "\n",
    "#     # Basic cleaning\n",
    "#     df = df.copy()\n",
    "#     # Trim strings\n",
    "#     for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "#         df[col] = df[col].astype(str).str.strip()\n",
    "#     # Drop duplicate rows\n",
    "#     df.drop_duplicates(inplace=True)\n",
    "\n",
    "#     # Type inference\n",
    "#     for col in df.columns:\n",
    "#         # Convert to numeric where possible\n",
    "#         if df[col].dtype == object:\n",
    "#             df[col] = pd.to_numeric(df[col], errors=\"ignore\")\n",
    "#         # Parse dates if looks like date\n",
    "#         if df[col].dtype == object and df[col].str.contains(r\"\\d{4}-\\d{2}-\\d{2}\", regex=True).any():\n",
    "#             try:\n",
    "#                 df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "\n",
    "#     # Missing value strategy: numeric -> median, categorical -> mode\n",
    "#     for col in df.columns:\n",
    "#         if df[col].dtype.kind in \"biufc\":\n",
    "#             df[col] = df[col].fillna(df[col].median())\n",
    "#         else:\n",
    "#             mode = df[col].mode(dropna=True)\n",
    "#             df[col] = df[col].fillna(mode.iloc[0] if not mode.empty else \"\")\n",
    "\n",
    "#     # Outlier capping (IQR) for numeric columns\n",
    "#     for col in df.select_dtypes(include=[np.number]).columns:\n",
    "#         q1, q3 = df[col].quantile([0.25, 0.75])\n",
    "#         iqr = q3 - q1\n",
    "#         if iqr > 0:\n",
    "#             low, high = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "#             df[col] = df[col].clip(lower=low, upper=high)\n",
    "\n",
    "#     # Save cleaned\n",
    "#     cleaned_path = os.path.join(out_dir, \"cleaned_data.csv\")\n",
    "#     df.to_csv(cleaned_path, index=False)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # ------------------ Relationships ------------------\n",
    "\n",
    "# def compute_relationships(df: pd.DataFrame, out_dir=\"outputs/relations\"):\n",
    "#     ensure_dir(out_dir)\n",
    "#     rel = {}\n",
    "\n",
    "#     # Correlations for numeric\n",
    "#     num_df = df.select_dtypes(include=[np.number])\n",
    "#     if not num_df.empty:\n",
    "#         corr = num_df.corr(numeric_only=True)\n",
    "#         corr_path = os.path.join(out_dir, \"correlation_matrix.csv\")\n",
    "#         corr.to_csv(corr_path)\n",
    "#         rel[\"correlation_matrix_path\"] = corr_path\n",
    "\n",
    "#     # Mutual information to a target if present (auto-pick last column candidate)\n",
    "#     target = None\n",
    "#     if df.columns.size > 1:\n",
    "#         target = df.columns[-1]\n",
    "#     rel[\"target_column_used\"] = target\n",
    "\n",
    "#     # Save a JSON summary\n",
    "#     with open(os.path.join(out_dir, \"relations_summary.json\"), \"w\") as f:\n",
    "#         json.dump(rel, f, indent=2)\n",
    "\n",
    "#     # Quick heatmap visualization\n",
    "#     if not num_df.empty:\n",
    "#         plt.figure(figsize=(10, 8))\n",
    "#         sns.heatmap(num_df.corr(numeric_only=True), cmap=\"coolwarm\", annot=False)\n",
    "#         plt.title(\"Correlation Heatmap\")\n",
    "#         plt.tight_layout()\n",
    "#         heatmap_path = os.path.join(out_dir, \"correlation_heatmap.png\")\n",
    "#         plt.savefig(heatmap_path, dpi=150)\n",
    "#         plt.close()\n",
    "\n",
    "#     return rel\n",
    "\n",
    "# # ------------------ Visualizations ------------------\n",
    "\n",
    "# def auto_visualize(df: pd.DataFrame, out_dir=\"outputs/visuals\"):\n",
    "#     ensure_dir(out_dir)\n",
    "#     # Histograms for numerics\n",
    "#     num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "#     for col in num_cols[:12]:\n",
    "#         plt.figure()\n",
    "#         sns.histplot(df[col].dropna(), kde=True)\n",
    "#         plt.title(f\"Distribution: {col}\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(os.path.join(out_dir, f\"dist_{safe_filename(col)}.png\"), dpi=150)\n",
    "#         plt.close()\n",
    "\n",
    "#     # Bar plots for top categorical\n",
    "#     cat_cols = df.select_dtypes(exclude=[np.number, \"datetime64[ns]\"]).columns.tolist()\n",
    "#     for col in cat_cols[:6]:\n",
    "#         plt.figure(figsize=(8,4))\n",
    "#         df[col].value_counts().head(20).plot(kind=\"bar\")\n",
    "#         plt.title(f\"Top categories: {col}\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(os.path.join(out_dir, f\"bar_{safe_filename(col)}.png\"), dpi=150)\n",
    "#         plt.close()\n",
    "\n",
    "# # ------------------ AutoML Training ------------------\n",
    "\n",
    "# def detect_task(df: pd.DataFrame, target: str):\n",
    "#     y = df[target]\n",
    "#     # Heuristic: classification if few unique values or non-numeric\n",
    "#     if y.dtype.kind not in \"biufc\" or y.nunique() <= max(20, int(0.05 * len(y))):\n",
    "#         return \"classification\"\n",
    "#     return \"regression\"\n",
    "\n",
    "# def train_automl(df: pd.DataFrame, target: str, out_dir=\"outputs/model\", time_limit_sec=300):\n",
    "#     ensure_dir(out_dir)\n",
    "#     X = df.drop(columns=[target])\n",
    "#     y = df[target]\n",
    "\n",
    "#     # Basic encoding for categoricals\n",
    "#     X_enc = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_enc, y, test_size=0.2, random_state=42)\n",
    "#     task = detect_task(df, target)\n",
    "\n",
    "#     best_score = None\n",
    "#     model = None\n",
    "\n",
    "#     if AUTOSKLEARN_AVAILABLE:\n",
    "#         if task == \"classification\":\n",
    "#             model = ask_cls.AutoSklearnClassifier(time_left_for_this_task=time_limit_sec, per_run_time_limit=60)\n",
    "#         else:\n",
    "#             model = ask_reg.AutoSklearnRegressor(time_left_for_this_task=time_limit_sec, per_run_time_limit=60)\n",
    "#         model.fit(X_train, y_train)\n",
    "#         preds = model.predict(X_test)\n",
    "#         score = accuracy_score(y_test, preds) if task == \"classification\" else r2_score(y_test, preds)\n",
    "#         best_score = score\n",
    "#     else:\n",
    "#         # Fallback simple model\n",
    "#         from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "#         if task == \"classification\":\n",
    "#             model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "#             model.fit(X_train, y_train)\n",
    "#             preds = model.predict(X_test)\n",
    "#             best_score = accuracy_score(y_test, preds)\n",
    "#         else:\n",
    "#             model = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "#             model.fit(X_train, y_train)\n",
    "#             preds = model.predict(X_test)\n",
    "#             best_score = r2_score(y_test, preds)\n",
    "\n",
    "#     joblib.dump({\"model\": model, \"columns\": X_enc.columns.tolist(), \"task\": task, \"target\": target}, os.path.join(out_dir, \"model.joblib\"))\n",
    "#     with open(os.path.join(out_dir, \"metrics.json\"), \"w\") as f:\n",
    "#         json.dump({\"task\": task, \"score\": best_score}, f, indent=2)\n",
    "#     return {\"task\": task, \"score\": best_score}\n",
    "\n",
    "# def predict_with_saved_model(model_dir: str, new_df: pd.DataFrame):\n",
    "#     bundle = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "#     model, cols, task, target = bundle[\"model\"], bundle[\"columns\"], bundle[\"task\"], bundle[\"target\"]\n",
    "#     X = new_df.drop(columns=[target], errors=\"ignore\")\n",
    "#     X_enc = pd.get_dummies(X, drop_first=True)\n",
    "#     # align\n",
    "#     for c in cols:\n",
    "#         if c not in X_enc.columns:\n",
    "#             X_enc[c] = 0\n",
    "#     X_enc = X_enc[cols]\n",
    "#     return model.predict(X_enc)\n",
    "\n",
    "# # ------------------ Ethical Web Mining ------------------\n",
    "\n",
    "# def allowed_by_robots(base_url: str, path: str=\"/\"):\n",
    "#     try:\n",
    "#         robots = requests.get(urljoin(base_url, \"/robots.txt\"), timeout=10)\n",
    "#         if robots.status_code != 200:\n",
    "#             return True  # default allow if no robots\n",
    "#         from urllib import robotparser\n",
    "#         rp = robotparser.RobotFileParser()\n",
    "#         rp.parse(robots.text.splitlines())\n",
    "#         return rp.can_fetch(\"*\", urljoin(base_url, path))\n",
    "#     except Exception:\n",
    "#         return False\n",
    "\n",
    "# def mine_public_table(start_url: str, max_rows=500, rate_sec=1.0):\n",
    "#     # Respect robots.txt; fetch a table from the page if present\n",
    "#     base = \"{uri.scheme}://{uri.netloc}\".format(uri=urlparse(start_url))\n",
    "#     if not allowed_by_robots(base, urlparse(start_url).path):\n",
    "#         raise PermissionError(\"robots.txt disallows scraping this path.\")\n",
    "#     time.sleep(rate_sec)\n",
    "#     r = requests.get(start_url, headers={\"User-Agent\": \"BusinessAnalystAgent/1.0\"}, timeout=15)\n",
    "#     r.raise_for_status()\n",
    "#     soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "#     table = soup.find(\"table\")\n",
    "#     if table is None:\n",
    "#         # Try to find links to CSV\n",
    "#         links = [a[\"href\"] for a in soup.find_all(\"a\", href=True)]\n",
    "#         csv_links = [l for l in links if l.lower().endswith(\".csv\")]\n",
    "#         if csv_links:\n",
    "#             csv_url = urljoin(start_url, csv_links[0])\n",
    "#             if not allowed_by_robots(base, urlparse(csv_url).path):\n",
    "#                 raise PermissionError(\"robots.txt disallows CSV path.\")\n",
    "#             time.sleep(rate_sec)\n",
    "#             return pd.read_csv(csv_url).head(max_rows)\n",
    "#         raise ValueError(\"No HTML table or CSV link found.\")\n",
    "#     # Parse HTML table\n",
    "#     df = pd.read_html(str(table))[0]\n",
    "#     return df.head(max_rows)\n",
    "\n",
    "# def save_mined_csv(df: pd.DataFrame, out_dir=\"outputs/mined\", name=\"mined_data.csv\"):\n",
    "#     ensure_dir(out_dir)\n",
    "#     p = os.path.join(out_dir, safe_filename(name))\n",
    "#     df.to_csv(p, index=False)\n",
    "#     return p\n",
    "\n",
    "# # ------------------ Orchestration ------------------\n",
    "\n",
    "# def run_agent(\n",
    "#     data_path_or_url: str | None = None,\n",
    "#     target: str | None = None,\n",
    "#     web_seed_url: str | None = None,\n",
    "#     time_limit_sec: int = 300\n",
    "# ):\n",
    "#     # Step 0: get or mine data\n",
    "#     if data_path_or_url:\n",
    "#         df_raw = load_data(data_path_or_url)\n",
    "#     else:\n",
    "#         if not web_seed_url:\n",
    "#             raise ValueError(\"No data provided. Provide web_seed_url to mine public data.\")\n",
    "#         df_raw = mine_public_table(web_seed_url)\n",
    "\n",
    "#     # Step 1: profile + clean\n",
    "#     df_clean = profile_and_clean(df_raw, out_dir=\"outputs/eda\", title=\"Business Analyst EDA Report\")\n",
    "\n",
    "#     # Step 2: relationships\n",
    "#     rel = compute_relationships(df_clean, out_dir=\"outputs/relations\")\n",
    "\n",
    "#     # Step 3: visuals\n",
    "#     auto_visualize(df_clean, out_dir=\"outputs/visuals\")\n",
    "\n",
    "#     # Step 4: train\n",
    "#     metrics = None\n",
    "#     if target and target in df_clean.columns:\n",
    "#         metrics = train_automl(df_clean, target=target, out_dir=\"outputs/model\", time_limit_sec=time_limit_sec)\n",
    "\n",
    "#     # Return pointers\n",
    "#     return {\n",
    "#         \"cleaned_csv\": \"outputs/eda/cleaned_data.csv\",\n",
    "#         \"eda_report_html\": \"outputs/eda/Business_Analyst_EDA_Report.html\",\n",
    "#         \"relations_summary\": \"outputs/relations/relations_summary.json\",\n",
    "#         \"visuals_dir\": \"outputs/visuals\",\n",
    "#         \"model_metrics\": metrics,\n",
    "#         \"model_dir\": \"outputs/model\" if metrics else None\n",
    "#     }\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example usage:\n",
    "#     # 1) With local or remote CSV\n",
    "#     # results = run_agent(data_path_or_url=\"data.csv\", target=\"SalePrice\")\n",
    "#     # 2) Without data: ethically mine a public table\n",
    "#     # results = run_agent(web_seed_url=\"https://www.worldometers.info/world-population/population-by-country/\")\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2fb7172-9f86-47fa-be9e-8f1537b0f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 4.0.0 Requires-Python >=3.7,<3.11; 4.1.0 Requires-Python >=3.7,<3.12; 4.1.1 Requires-Python >=3.7,<3.12; 4.1.2 Requires-Python >=3.7,<3.12; 4.10.0 Requires-Python >=3.7,<3.13; 4.11.0 Requires-Python >=3.7,<3.13; 4.12.0 Requires-Python >=3.7,<3.13; 4.12.1 Requires-Python >=3.7,<3.13; 4.12.2 Requires-Python >=3.7,<3.13; 4.13.0 Requires-Python >=3.7,<3.13; 4.14.0 Requires-Python >=3.7,<3.13; 4.15.0 Requires-Python >=3.7,<3.13; 4.15.1 Requires-Python >=3.7,<3.13; 4.16.0 Requires-Python >=3.7,<3.13; 4.16.1 Requires-Python >=3.7,<3.13; 4.2.0 Requires-Python >=3.7,<3.12; 4.3.0 Requires-Python >=3.7,<3.12; 4.3.1 Requires-Python >=3.7,<3.12; 4.3.2 Requires-Python >=3.7,<3.12; 4.4.0 Requires-Python >=3.7,<3.12; 4.5.0 Requires-Python >=3.7,<3.12; 4.5.1 Requires-Python >=3.7,<3.12; 4.6.0 Requires-Python >=3.7,<3.12; 4.6.1 Requires-Python >=3.7,<3.12; 4.6.2 Requires-Python >=3.7,<3.12; 4.6.3 Requires-Python >=3.7,<3.12; 4.6.4 Requires-Python >=3.7,<3.12; 4.6.5 Requires-Python >=3.7,<3.12; 4.7.0 Requires-Python >=3.7,<3.13; 4.8.3 Requires-Python >=3.7,<3.13; 4.9.0 Requires-Python >=3.7,<3.13\n",
      "ERROR: Could not find a version that satisfies the requirement ydata-profiling (from versions: none)\n",
      "ERROR: No matching distribution found for ydata-profiling\n"
     ]
    }
   ],
   "source": [
    "# !pip install ydata-profiling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4917ff56-0afb-4756-a474-c22643a3e186",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2339512383.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install streamlit\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# pip install streamlit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ee11ef-fc60-4952-9a35-8fcdd2374c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting altair!=5.4.0,!=5.4.1,<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.5.0 (from streamlit)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<7,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-6.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from streamlit) (2.3.3)\n",
      "Requirement already satisfied: packaging<26,>=20 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from streamlit) (25.0)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from streamlit) (2.3.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from streamlit) (10.4.0)\n",
      "Collecting protobuf<7,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-6.32.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-21.0.0-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from streamlit) (2.32.5)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from streamlit) (4.15.0)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from streamlit) (6.5.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.0)\n",
      "Collecting narwhals>=1.14.2 (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-2.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.22.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 2.6/10.0 MB 13.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.4/10.0 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.2/10.0 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.0/10.0 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.8/10.0 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.0/10.0 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.3/10.0 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.6/10.0 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.8/10.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.3/10.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.9/10.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.7/10.0 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.0 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 3.4 MB/s  0:00:02\n",
      "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "   ---------------------------------------- 0.0/731.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 731.2/731.2 kB 4.1 MB/s  0:00:00\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-6.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading protobuf-6.32.1-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.0/6.9 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.1/6.9 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.1/6.9 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.2/6.9 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.2/6.9 MB 5.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.8/6.9 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.3/6.9 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/6.9 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.9/6.9 MB 3.7 MB/s  0:00:01\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Downloading narwhals-2.5.0-py3-none-any.whl (407 kB)\n",
      "Downloading pyarrow-21.0.0-cp313-cp313-win_amd64.whl (26.1 MB)\n",
      "   ---------------------------------------- 0.0/26.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/26.1 MB 2.6 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 1.0/26.1 MB 2.9 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 1.8/26.1 MB 3.2 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 2.6/26.1 MB 3.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.7/26.1 MB 3.7 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 4.7/26.1 MB 4.0 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.8/26.1 MB 4.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 7.1/26.1 MB 4.5 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 7.9/26.1 MB 4.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 8.4/26.1 MB 4.3 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.9/26.1 MB 4.0 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.2/26.1 MB 3.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.7/26.1 MB 3.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 10.5/26.1 MB 3.7 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 11.3/26.1 MB 3.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 12.1/26.1 MB 3.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 13.1/26.1 MB 3.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 14.2/26.1 MB 3.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 15.5/26.1 MB 4.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 16.5/26.1 MB 4.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 17.0/26.1 MB 4.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 17.3/26.1 MB 3.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 17.8/26.1 MB 3.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 18.1/26.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 18.6/26.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 19.1/26.1 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 19.7/26.1 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 20.4/26.1 MB 3.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 21.2/26.1 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.0/26.1 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 23.1/26.1 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.3/26.1 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 24.1/26.1 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.9/26.1 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.4/26.1 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.0/26.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.1/26.1 MB 3.5 MB/s  0:00:07\n",
      "Installing collected packages: watchdog, toml, tenacity, smmap, pyarrow, protobuf, narwhals, click, cachetools, blinker, pydeck, gitdb, gitpython, altair, streamlit\n",
      "\n",
      "   -- -------------------------------------  1/15 [toml]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ---------- -----------------------------  4/15 [pyarrow]\n",
      "   ------------- --------------------------  5/15 [protobuf]\n",
      "   ------------- --------------------------  5/15 [protobuf]\n",
      "   ------------- --------------------------  5/15 [protobuf]\n",
      "   ---------------- -----------------------  6/15 [narwhals]\n",
      "   ---------------- -----------------------  6/15 [narwhals]\n",
      "   ---------------- -----------------------  6/15 [narwhals]\n",
      "   ---------------- -----------------------  6/15 [narwhals]\n",
      "   ---------------- -----------------------  6/15 [narwhals]\n",
      "   ---------------- -----------------------  6/15 [narwhals]\n",
      "   ------------------ ---------------------  7/15 [click]\n",
      "   --------------------- ------------------  8/15 [cachetools]\n",
      "   -------------------------- ------------- 10/15 [pydeck]\n",
      "   -------------------------- ------------- 10/15 [pydeck]\n",
      "   ----------------------------- ---------- 11/15 [gitdb]\n",
      "   -------------------------------- ------- 12/15 [gitpython]\n",
      "   -------------------------------- ------- 12/15 [gitpython]\n",
      "   ---------------------------------- ----- 13/15 [altair]\n",
      "   ---------------------------------- ----- 13/15 [altair]\n",
      "   ---------------------------------- ----- 13/15 [altair]\n",
      "   ---------------------------------- ----- 13/15 [altair]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ------------------------------------- -- 14/15 [streamlit]\n",
      "   ---------------------------------------- 15/15 [streamlit]\n",
      "\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 cachetools-6.2.0 click-8.2.1 gitdb-4.0.12 gitpython-3.1.45 narwhals-2.5.0 protobuf-6.32.1 pyarrow-21.0.0 pydeck-0.9.1 smmap-5.0.2 streamlit-1.49.1 tenacity-9.1.2 toml-0.10.2 watchdog-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03bc2db-03c0-47ce-a810-c416e286ed51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 18:52:54.523 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.525 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.525 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.527 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.532 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.537 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.538 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.539 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.541 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.542 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.544 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.545 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.546 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.548 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.552 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-17 18:52:54.554 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# # app.py\n",
    "# import streamlit as st\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import plotly.express as px\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.metrics import r2_score, mean_absolute_error, accuracy_score, f1_score\n",
    "# from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "# from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "# from sklearn.feature_selection import chi2\n",
    "# from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "# from sklearn.utils.multiclass import type_of_target\n",
    "# from category_encoders.target_encoder import TargetEncoder\n",
    "# import shap\n",
    "# import joblib\n",
    "# import os\n",
    "\n",
    "# st.set_page_config(page_title=\"Business Analyst Auto-App\", layout=\"wide\")\n",
    "\n",
    "# st.title(\"Business Analyst Auto-App (No Auto-Sklearn / No YData-Profiling)\")\n",
    "\n",
    "# # ---------- Helpers ----------\n",
    "# def infer_types(df):\n",
    "#     df_copy = df.copy()\n",
    "#     for col in df_copy.columns:\n",
    "#         # try numeric conversion where possible\n",
    "#         if df_copy[col].dtype == object:\n",
    "#             try:\n",
    "#                 df_copy[col] = pd.to_numeric(df_copy[col])\n",
    "#             except:\n",
    "#                 pass\n",
    "#         # try datetime\n",
    "#         if df_copy[col].dtype == object:\n",
    "#             try:\n",
    "#                 df_copy[col] = pd.to_datetime(df_copy[col])\n",
    "#             except:\n",
    "#                 pass\n",
    "#     return df_copy\n",
    "\n",
    "# def basic_clean(df):\n",
    "#     df = df.copy()\n",
    "#     # Drop exact duplicate rows\n",
    "#     df = df.drop_duplicates()\n",
    "#     # Remove columns with all nulls\n",
    "#     df = df.dropna(axis=1, how='all')\n",
    "#     # Trim whitespace in object columns\n",
    "#     for c in df.select_dtypes(include=['object']).columns:\n",
    "#         df[c] = df[c].astype(str).str.strip()\n",
    "#     return df\n",
    "\n",
    "# def split_cols(df):\n",
    "#     num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "#     cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "#     # treat datetimes as categorical for quick EDA; could extract parts\n",
    "#     dt_cols = df.select_dtypes(include=['datetime64[ns]']).columns.tolist()\n",
    "#     return num_cols, cat_cols, dt_cols\n",
    "\n",
    "# def encode_datetime(df, dt_cols):\n",
    "#     # expand datetime to parts\n",
    "#     df = df.copy()\n",
    "#     for c in dt_cols:\n",
    "#         df[c+\"_year\"] = df[c].dt.year\n",
    "#         df[c+\"_month\"] = df[c].dt.month\n",
    "#         df[c+\"_day\"] = df[c].dt.day\n",
    "#         df[c+\"_dow\"] = df[c].dt.dayofweek\n",
    "#         df[c+\"_hour\"] = df[c].dt.hour\n",
    "#     df = df.drop(columns=dt_cols)\n",
    "#     return df\n",
    "\n",
    "# def build_preprocessor(num_cols, cat_cols, use_target_encoder=False):\n",
    "#     num_pipe = Pipeline(steps=[\n",
    "#         (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "#         (\"scale\", StandardScaler())\n",
    "#     ])\n",
    "#     if use_target_encoder:\n",
    "#         cat_pipe = Pipeline(steps=[\n",
    "#             (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "#             (\"encode\", TargetEncoder())\n",
    "#         ])\n",
    "#     else:\n",
    "#         cat_pipe = Pipeline(steps=[\n",
    "#             (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "#             (\"encode\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "#         ])\n",
    "#     pre = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             (\"num\", num_pipe, num_cols),\n",
    "#             (\"cat\", cat_pipe, cat_cols)\n",
    "#         ],\n",
    "#         remainder=\"drop\"\n",
    "#     )\n",
    "#     return pre\n",
    "\n",
    "# def detect_task_type(y):\n",
    "#     t = type_of_target(y)\n",
    "#     if t in [\"binary\", \"multiclass\"]:\n",
    "#         return \"classification\"\n",
    "#     return \"regression\"\n",
    "\n",
    "# def compute_relationships(df, target=None):\n",
    "#     # Correlations (numeric)\n",
    "#     num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "#     corr = None\n",
    "#     if len(num_cols) >= 2:\n",
    "#         corr = df[num_cols].corr(method=\"spearman\")\n",
    "#     # MI\n",
    "#     mi = None\n",
    "#     if target and target in df.columns:\n",
    "#         y = df[target]\n",
    "#         X = df.drop(columns=[target])\n",
    "#         # quick simplification: drop datetimes if any\n",
    "#         X = X.select_dtypes(exclude=[\"datetime64[ns]\"]).copy()\n",
    "#         y_type = detect_task_type(y)\n",
    "#         # coerce non-numeric categorical to codes for MI\n",
    "#         for c in X.select_dtypes(include=['object', 'category']).columns:\n",
    "#             X[c] = X[c].astype('category').cat.codes\n",
    "#         if y_type == \"classification\":\n",
    "#             if y.dtype.kind not in ['i','u']:\n",
    "#                 y = y.astype('category').cat.codes\n",
    "#             mi_scores = mutual_info_classif(X, y, discrete_features='auto', random_state=42)\n",
    "#         else:\n",
    "#             y = pd.to_numeric(y, errors=\"coerce\")\n",
    "#             mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "#         mi = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "#     return corr, mi\n",
    "\n",
    "# def train_quick_model(df, target):\n",
    "#     y = df[target]\n",
    "#     X = df.drop(columns=[target])\n",
    "#     # keep simple: expand datetimes\n",
    "#     _, _, dt_cols = split_cols(X)\n",
    "#     X = encode_datetime(X, dt_cols)\n",
    "#     num_cols, cat_cols, _ = split_cols(X)\n",
    "#     task = detect_task_type(y)\n",
    "#     pre = build_preprocessor(num_cols, cat_cols, use_target_encoder=(task==\"regression\"))\n",
    "#     if task == \"classification\":\n",
    "#         model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "#     else:\n",
    "#         model = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "#     pipe = Pipeline(steps=[(\"pre\", pre), (\"model\", model)])\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if task==\"classification\" else None)\n",
    "#     pipe.fit(X_train, y_train)\n",
    "#     preds = pipe.predict(X_test)\n",
    "#     metrics = {}\n",
    "#     if task == \"classification\":\n",
    "#         metrics[\"accuracy\"] = accuracy_score(y_test, preds)\n",
    "#         metrics[\"f1\"] = f1_score(y_test, preds, average=\"weighted\")\n",
    "#     else:\n",
    "#         metrics[\"r2\"] = r2_score(y_test, preds)\n",
    "#         metrics[\"mae\"] = mean_absolute_error(y_test, preds)\n",
    "#     return pipe, metrics, task\n",
    "\n",
    "# def shap_importance(pipe, X_sample, max_display=10):\n",
    "#     try:\n",
    "#         # Extract underlying estimator if in Pipeline\n",
    "#         model = pipe.named_steps[\"model\"]\n",
    "#         pre = pipe.named_steps[\"pre\"]\n",
    "#         X_trans = pre.transform(X_sample)\n",
    "#         explainer = shap.TreeExplainer(model)\n",
    "#         shap_values = explainer.shap_values(X_trans)\n",
    "#         # Feature names after transform\n",
    "#         try:\n",
    "#             feature_names = pre.get_feature_names_out()\n",
    "#         except:\n",
    "#             feature_names = [f\"f_{i}\" for i in range(X_trans.shape[1])]\n",
    "#         if isinstance(shap_values, list):  # classification list per class\n",
    "#             vals = np.abs(shap_values[0]).mean(axis=0)\n",
    "#         else:\n",
    "#             vals = np.abs(shap_values).mean(axis=0)\n",
    "#         imp = pd.Series(vals, index=feature_names).sort_values(ascending=False).head(max_display)\n",
    "#         return imp\n",
    "#     except Exception as e:\n",
    "#         return pd.Series(dtype=float)\n",
    "\n",
    "# def incremental_fit(df, target, cache_dir=\"models\"):\n",
    "#     os.makedirs(cache_dir, exist_ok=True)\n",
    "#     y = df[target]\n",
    "#     X = df.drop(columns=[target])\n",
    "#     _, _, dt_cols = split_cols(X)\n",
    "#     X = encode_datetime(X, dt_cols)\n",
    "#     num_cols, cat_cols, _ = split_cols(X)\n",
    "#     task = detect_task_type(y)\n",
    "#     if task == \"classification\":\n",
    "#         base = SGDClassifier(loss=\"log_loss\", random_state=42)\n",
    "#     else:\n",
    "#         base = SGDRegressor(random_state=42)\n",
    "#     pre = build_preprocessor(num_cols, cat_cols, use_target_encoder=(task==\"regression\"))\n",
    "#     pipe = Pipeline(steps=[(\"pre\", pre), (\"model\", base)])\n",
    "#     try:\n",
    "#         pipe = joblib.load(os.path.join(cache_dir, \"online.pkl\"))\n",
    "#     except:\n",
    "#         pass\n",
    "#     # Simple partial fit loop (convert y for classification if needed)\n",
    "#     if task == \"classification\" and y.dtype.kind not in ['i','u']:\n",
    "#         y_codes = y.astype('category').cat.codes\n",
    "#         classes = np.unique(y_codes)\n",
    "#         pipe.named_steps[\"model\"].partial_fit(pre.fit_transform(X), y_codes, classes=classes)\n",
    "#     else:\n",
    "#         y_num = pd.to_numeric(y, errors=\"coerce\")\n",
    "#         m = pipe.named_steps[\"model\"]\n",
    "#         Xt = pre.fit_transform(X)\n",
    "#         # mini-batch updates\n",
    "#         for i in range(0, Xt.shape[0], 512):\n",
    "#             m.partial_fit(Xt[i:i+512], y_num.iloc[i:i+512])\n",
    "#     joblib.dump(pipe, os.path.join(cache_dir, \"online.pkl\"))\n",
    "#     return \"Model updated\"\n",
    "\n",
    "# # ---------- UI ----------\n",
    "# uploaded = st.file_uploader(\"Upload a CSV\", type=[\"csv\"])\n",
    "# target = st.text_input(\"Optional: enter target column for modeling (for SHAP and suggestions)\")\n",
    "\n",
    "# if uploaded is not None:\n",
    "#     df_raw = pd.read_csv(uploaded)\n",
    "#     st.subheader(\"Raw preview\")\n",
    "#     st.dataframe(df_raw.head(20))\n",
    "\n",
    "#     df1 = infer_types(df_raw)\n",
    "#     df2 = basic_clean(df1)\n",
    "#     st.subheader(\"After type inference and basic cleaning\")\n",
    "#     st.write(f\"Rows: {df2.shape[0]}, Cols: {df2.shape[1]}\")\n",
    "#     st.dataframe(df2.head(20))\n",
    "\n",
    "#     # Datetime expansion for EDA charts\n",
    "#     num_cols, cat_cols, dt_cols = split_cols(df2)\n",
    "#     df_eda = encode_datetime(df2, dt_cols)\n",
    "\n",
    "#     # Relationships\n",
    "#     st.subheader(\"Relationships\")\n",
    "#     corr, mi = compute_relationships(df2, target=target if target in df2.columns else None)\n",
    "#     if corr is not None:\n",
    "#         st.write(\"Spearman correlation (numeric):\")\n",
    "#         st.dataframe(corr.round(3))\n",
    "#         if len(corr.columns) > 1:\n",
    "#             fig = px.imshow(corr, color_continuous_scale=\"Blues\", title=\"Correlation Heatmap\")\n",
    "#             st.plotly_chart(fig, use_container_width=True)\n",
    "#     if mi is not None:\n",
    "#         st.write(\"Mutual Information with target:\")\n",
    "#         st.dataframe(mi.to_frame(\"MI\").round(4))\n",
    "\n",
    "#     # Visualizations\n",
    "#     st.subheader(\"Quick Visuals\")\n",
    "#     # Numeric hist\n",
    "#     if len(num_cols) > 0:\n",
    "#         sel_num = st.selectbox(\"Numeric column for histogram\", options=num_cols)\n",
    "#         fig = px.histogram(df2, x=sel_num, nbins=40)\n",
    "#         st.plotly_chart(fig, use_container_width=True)\n",
    "#     # Category count\n",
    "#     if len(cat_cols) > 0:\n",
    "#         sel_cat = st.selectbox(\"Categorical column for counts\", options=cat_cols)\n",
    "#         fig = px.bar(df2[sel_cat].value_counts().reset_index(), x=\"index\", y=sel_cat)\n",
    "#         st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "#     # Modeling + SHAP\n",
    "#     if target and target in df2.columns:\n",
    "#         st.subheader(\"Auto Model + SHAP\")\n",
    "#         pipe, metrics, task = train_quick_model(df2, target)\n",
    "#         st.write(f\"Task: {task}, Metrics: {metrics}\")\n",
    "#         # SHAP top features\n",
    "#         X_sample = df2.drop(columns=[target]).sample(min(500, df2.shape[0]), random_state=42)\n",
    "#         imp = shap_importance(pipe, X_sample)\n",
    "#         if not imp.empty:\n",
    "#             st.write(\"Top features by SHAP (mean |value|):\")\n",
    "#             st.dataframe(imp.to_frame(\"importance\").round(5))\n",
    "#             fig = px.bar(imp.sort_values(ascending=True), orientation=\"h\", title=\"SHAP mean |importance|\")\n",
    "#             st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "#         # Suggestions (rule + model-driven)\n",
    "#         st.subheader(\"Decision Suggestions\")\n",
    "#         suggestions = []\n",
    "#         # Example heuristic: if MI shows strong drivers, suggest focusing\n",
    "#         if mi is not None and not mi.empty:\n",
    "#             top_driver = mi.index[0]\n",
    "#             suggestions.append(f\"Investigate segments by '{top_driver}' for targeted campaigns or pricing, as it shows highest dependency with the target.\")\n",
    "#         # If classification and low accuracy, suggest data improvements\n",
    "#         if task == \"classification\" and metrics.get(\"accuracy\", 1) < 0.8:\n",
    "#             suggestions.append(\"Model accuracy is below 0.8; consider collecting more labeled data, balancing classes, or enriching features.\")\n",
    "#         # If regression and low R2\n",
    "#         if task == \"regression\" and metrics.get(\"r2\", 1) < 0.6:\n",
    "#             suggestions.append(\"Predictive power is modest; explore additional business drivers, lag features, seasonality, or external benchmarks.\")\n",
    "#         # If categorical cardinality high\n",
    "#         high_card = [c for c in cat_cols if df2[c].nunique() > 100]\n",
    "#         if high_card:\n",
    "#             suggestions.append(f\"High-cardinality categories detected ({', '.join(high_card[:3])}). Consider grouping or target encoding for better generalization.\")\n",
    "#         # General BI actions\n",
    "#         suggestions.append(\"Build segment dashboards for top 3 drivers and monitor weekly KPIs to detect drift and outliers.\")\n",
    "#         suggestions.append(\"Run A/B tests on offers or pricing in high-impact segments indicated by SHAP to validate uplift.\")\n",
    "#         st.write(\"- \" + \"\\n- \".join(suggestions))\n",
    "\n",
    "#         # Online learning button\n",
    "#         if st.button(\"Incrementally train for future data (online)\"):\n",
    "#             msg = incremental_fit(df2, target)\n",
    "#             st.success(msg)\n",
    "\n",
    "#     else:\n",
    "#         st.info(\"Enter a valid target column to enable modeling, SHAP, and decision suggestions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b3b519-e9e4-49fb-a787-96250a0e0baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting jupyter\n",
      "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipywidgets) (9.1.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting notebook (from jupyter)\n",
      "  Downloading notebook-7.4.5-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jupyter-console (from jupyter)\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter) (6.30.1)\n",
      "Requirement already satisfied: jupyterlab in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter) (4.4.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipykernel->jupyter) (1.8.16)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipykernel->jupyter) (5.8.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipykernel->jupyter) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipykernel->jupyter) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipykernel->jupyter) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from ipykernel->jupyter) (6.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-client>=8.0.0->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.3.7)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (311)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel->jupyter) (1.17.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyterlab->jupyter) (2.0.4)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyterlab->jupyter) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyterlab->jupyter) (3.1.6)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyterlab->jupyter) (2.2.5)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyterlab->jupyter) (2.16.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyterlab->jupyter) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyterlab->jupyter) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyterlab->jupyter) (78.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (21.3.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.21.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.0.15)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.16.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.9.25)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.25.0)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from anyio->httpx<1,>=0.25.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (25.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jinja2>=3.0.3->jupyterlab->jupyter) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.22.3)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rfc3987-syntax>=1.1.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading rfc3987_syntax-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from nbconvert->jupyter) (4.13.5)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from nbconvert->jupyter) (3.1.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from nbconvert->jupyter) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.20.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.5.0)\n",
      "Collecting lark>=1.2.2 (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.23)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter) (4.15.0)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading types_python_dateutil-2.9.0.20250822-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in c:\\users\\udit\\anaconda3\\envs\\sklearn\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 16.6 MB/s  0:00:00\n",
      "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)\n",
      "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Downloading webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Downloading types_python_dateutil-2.9.0.20250822-py3-none-any.whl (17 kB)\n",
      "Downloading notebook-7.4.5-py3-none-any.whl (14.3 MB)\n",
      "   ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 2.1/14.3 MB 9.8 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.6/14.3 MB 7.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.1/14.3 MB 5.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.4/14.3 MB 4.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.7/14.3 MB 3.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.2/14.3 MB 3.3 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 4.7/14.3 MB 3.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 5.5/14.3 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 6.3/14.3 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 7.1/14.3 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 8.1/14.3 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 9.2/14.3 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 10.5/14.3 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.3/14.3 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.8/14.3 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.1/14.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.8/14.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.4/14.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.3/14.3 MB 3.6 MB/s  0:00:03\n",
      "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: widgetsnbextension, webcolors, uri-template, types-python-dateutil, lark, jupyterlab_widgets, jsonpointer, fqdn, rfc3987-syntax, arrow, isoduration, ipywidgets, jupyter-console, notebook, jupyter\n",
      "\n",
      "   ---------- -----------------------------  4/15 [lark]\n",
      "   ---------- -----------------------------  4/15 [lark]\n",
      "   ------------------------ ---------------  9/15 [arrow]\n",
      "   ----------------------------- ---------- 11/15 [ipywidgets]\n",
      "   ----------------------------- ---------- 11/15 [ipywidgets]\n",
      "   ---------------------------------- ----- 13/15 [notebook]\n",
      "   ---------------------------------- ----- 13/15 [notebook]\n",
      "   ---------------------------------- ----- 13/15 [notebook]\n",
      "   ---------------------------------- ----- 13/15 [notebook]\n",
      "   ---------------------------------------- 15/15 [jupyter]\n",
      "\n",
      "Successfully installed arrow-1.3.0 fqdn-1.5.1 ipywidgets-8.1.7 isoduration-20.11.0 jsonpointer-3.0.0 jupyter-1.1.1 jupyter-console-6.6.3 jupyterlab_widgets-3.0.15 lark-1.2.2 notebook-7.4.5 rfc3987-syntax-1.1.0 types-python-dateutil-2.9.0.20250822 uri-template-1.3.0 webcolors-24.11.1 widgetsnbextension-4.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install --upgrade ipywidgets jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e2a52c-8d71-4662-b5ab-b29ebfaa0a78",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (507122745.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mstreamlit run app.py\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# streamlit run app.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cee69d-975f-4690-b3a9-ee48fb8bb918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
